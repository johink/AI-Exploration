{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Single model:  \n",
    "  * 97.05% accuracy\n",
    "* 7 model ensemble:  \n",
    "  * 98.79% accuracy\n",
    "* SWA model:\n",
    "  * 98.76% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John\\Anaconda3\\envs\\retro\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-71e12f4bac70>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\John\\Anaconda3\\envs\\retro\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\John\\Anaconda3\\envs\\retro\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\John\\Anaconda3\\envs\\retro\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\John\\Anaconda3\\envs\\retro\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\John\\Anaconda3\\envs\\retro\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "784 ** .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainx = mnist.train.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testx = mnist.test.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainy = mnist.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testy = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 784), (10000, 784), (55000, 10), (10000, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx.shape, testx.shape, trainy.shape, testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx = trainx.reshape(-1, 1, 28, 28)\n",
    "testx = testx.reshape(-1, 1, 28, 28)\n",
    "trainx.shape, testx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel_size, padding, stride = 1, num_resid = 3):\n",
    "        super(ResLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_dim, out_dim, kernel_size, padding = padding, stride = stride)\n",
    "        self.res = nn.Conv2d(out_dim, out_dim, kernel_size, padding = padding)\n",
    "        self.num_resid = num_resid\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        for _ in range(self.num_resid):\n",
    "            x_conv = F.relu(self.res(x))\n",
    "            x = F.relu(x + self.res(x_conv))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.conv1 = ResLayer(1, 32, 5, padding = 2)\n",
    "        self.drop1 = nn.Dropout(.2)\n",
    "        self.conv2 = ResLayer(32, 64, 5, padding = 2, stride = 2)\n",
    "        self.drop2 = nn.Dropout(.3)\n",
    "        self.conv3 = ResLayer(64, 128, 5, padding = 2, stride = 2)\n",
    "        self.drop3 = nn.Dropout(.5)\n",
    "        self.conv_out = nn.Conv2d(128, 10, 7, padding = 0)\n",
    "        \n",
    "        self.layers = nn.Sequential(self.conv1, self.drop1,\n",
    "                                    self.conv2, self.drop2,\n",
    "                                    self.conv3, self.drop3,\n",
    "                                    self.conv_out, nn.Softmax(1))\n",
    "    \n",
    "    def forward(self, images):\n",
    "        return self.layers(images).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1022,  0.0971,  0.0987,  0.1006,  0.0980,  0.1122,  0.0902,\n",
       "          0.1061,  0.0954,  0.0996],\n",
       "        [ 0.1048,  0.0930,  0.1049,  0.1007,  0.0992,  0.1079,  0.0989,\n",
       "          0.0974,  0.0958,  0.0974],\n",
       "        [ 0.1008,  0.1020,  0.1021,  0.1045,  0.1049,  0.0998,  0.0961,\n",
       "          0.0928,  0.0990,  0.0980],\n",
       "        [ 0.1060,  0.0939,  0.0947,  0.0964,  0.1028,  0.1092,  0.0988,\n",
       "          0.0962,  0.1016,  0.1004],\n",
       "        [ 0.0985,  0.0925,  0.0985,  0.1034,  0.1012,  0.1116,  0.1012,\n",
       "          0.0950,  0.0972,  0.1008]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MNISTNet()\n",
    "model(torch.FloatTensor(trainx[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_data_generator(trainx, trainy, batch_size = 32, subsample_pct = 1):\n",
    "    n_samples = trainx.shape[0]\n",
    "    indexes = np.random.permutation(n_samples)\n",
    "    n_samples_sub = int(n_samples * subsample_pct)\n",
    "    indexes = indexes[:n_samples_sub]\n",
    "    \n",
    "    for i in range(ceil(n_samples_sub / batch_size)):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        yield_indices = indexes[start:end]\n",
    "        yield trainx[yield_indices], trainy[yield_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, epoch, start_lr = .001):\n",
    "    lr = start_lr * (0.9 ** epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MNISTNet()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = torch.FloatTensor(testx)\n",
    "test_data_labels = torch.LongTensor(testy.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWA Run 0\n",
      "0: LR = 0.0010\n",
      "Training loss: 1.5259553914846375\n",
      "Testing loss: 1.4909248888492583\n",
      "1: LR = 0.0009\n",
      "Training loss: 1.510747754296591\n",
      "Testing loss: 1.494855946302414\n",
      "2: LR = 0.0008\n",
      "Training loss: 1.5038599690725638\n",
      "Testing loss: 1.4875292301177978\n",
      "3: LR = 0.0007\n",
      "Training loss: 1.507502134456191\n",
      "Testing loss: 1.4904719173908234\n",
      "4: LR = 0.0007\n",
      "Training loss: 1.5016566792199777\n",
      "Testing loss: 1.4931854367256165\n",
      "5: LR = 0.0006\n",
      "Training loss: 1.491364789563556\n",
      "Testing loss: 1.4880838990211487\n",
      "6: LR = 0.0005\n",
      "Training loss: 1.491385446038357\n",
      "Testing loss: 1.4863834857940674\n",
      "7: LR = 0.0005\n",
      "Training loss: 1.48805673732314\n",
      "Testing loss: 1.4829363763332366\n",
      "8: LR = 0.0004\n",
      "Training loss: 1.4890346721161243\n",
      "Testing loss: 1.4760433971881866\n",
      "9: LR = 0.0004\n",
      "Training loss: 1.4890114651169888\n",
      "Testing loss: 1.4812898695468903\n",
      "SWA Run 1\n",
      "0: LR = 0.0010\n",
      "Training loss: 1.4937098442121994\n",
      "Testing loss: 1.495544159412384\n",
      "1: LR = 0.0009\n",
      "Training loss: 1.5083259898562764\n",
      "Testing loss: 1.5043830811977386\n",
      "2: LR = 0.0008\n",
      "Training loss: 1.5011517779771673\n",
      "Testing loss: 1.4873688697814942\n",
      "3: LR = 0.0007\n",
      "Training loss: 1.498062158739844\n",
      "Testing loss: 1.503383708000183\n",
      "4: LR = 0.0007\n",
      "Training loss: 1.4967483670212502\n",
      "Testing loss: 1.4919494807720184\n",
      "5: LR = 0.0006\n",
      "Training loss: 1.4963438566340956\n",
      "Testing loss: 1.489808440208435\n",
      "6: LR = 0.0005\n",
      "Training loss: 1.4957038496815882\n",
      "Testing loss: 1.4824113965034484\n",
      "7: LR = 0.0005\n",
      "Training loss: 1.490898049154947\n",
      "Testing loss: 1.479623144865036\n",
      "8: LR = 0.0004\n",
      "Training loss: 1.4880275864933812\n",
      "Testing loss: 1.475678378343582\n",
      "9: LR = 0.0004\n",
      "Training loss: 1.4883586695027906\n",
      "Testing loss: 1.4823876857757567\n",
      "SWA Run 2\n",
      "0: LR = 0.0010\n",
      "Training loss: 1.506403124609659\n",
      "Testing loss: 1.493806892633438\n",
      "1: LR = 0.0009\n",
      "Training loss: 1.4975618817085443\n",
      "Testing loss: 1.4797371566295623\n",
      "2: LR = 0.0008\n",
      "Training loss: 1.4945870998293855\n",
      "Testing loss: 1.4939724266529084\n",
      "3: LR = 0.0007\n",
      "Training loss: 1.5056805083918017\n",
      "Testing loss: 1.491559273004532\n",
      "4: LR = 0.0007\n",
      "Training loss: 1.4926254582959553\n",
      "Testing loss: 1.4890987873077393\n",
      "5: LR = 0.0006\n",
      "Training loss: 1.497611256532891\n",
      "Testing loss: 1.4919697999954225\n",
      "6: LR = 0.0005\n",
      "Training loss: 1.487702918607135\n",
      "Testing loss: 1.4774643301963806\n",
      "7: LR = 0.0005\n",
      "Training loss: 1.481458580771158\n",
      "Testing loss: 1.4774449586868286\n",
      "8: LR = 0.0004\n",
      "Training loss: 1.4798848129982172\n",
      "Testing loss: 1.4761463761329652\n",
      "9: LR = 0.0004\n",
      "Training loss: 1.4824797497239224\n",
      "Testing loss: 1.4781343340873718\n",
      "SWA Run 3\n",
      "0: LR = 0.0010\n",
      "Training loss: 1.4840534481891365\n",
      "Testing loss: 1.4793240427970886\n",
      "1: LR = 0.0009\n",
      "Training loss: 1.4876121615254603\n",
      "Testing loss: 1.4854330122470856\n",
      "2: LR = 0.0008\n",
      "Training loss: 1.4861180671425753\n",
      "Testing loss: 1.4880278408527374\n",
      "3: LR = 0.0007\n",
      "Training loss: 1.4885358893594076\n",
      "Testing loss: 1.4839212357997895\n",
      "4: LR = 0.0007\n",
      "Training loss: 1.488275877265043\n",
      "Testing loss: 1.483052098751068\n",
      "5: LR = 0.0006\n",
      "Training loss: 1.4838431186454242\n",
      "Testing loss: 1.4793982326984405\n",
      "6: LR = 0.0005\n",
      "Training loss: 1.4820877285890801\n",
      "Testing loss: 1.4832349240779876\n",
      "7: LR = 0.0005\n",
      "Training loss: 1.4814253906870998\n",
      "Testing loss: 1.4758622705936433\n",
      "8: LR = 0.0004\n",
      "Training loss: 1.4788147626921189\n",
      "Testing loss: 1.48126562833786\n",
      "9: LR = 0.0004\n",
      "Training loss: 1.4795095005700754\n",
      "Testing loss: 1.4732749998569488\n",
      "SWA Run 4\n",
      "0: LR = 0.0010\n",
      "Training loss: 1.4869506608608156\n",
      "Testing loss: 1.4827898025512696\n",
      "1: LR = 0.0009\n",
      "Training loss: 1.490592310594958\n",
      "Testing loss: 1.4842418015003205\n",
      "2: LR = 0.0008\n",
      "Training loss: 1.4887487611105277\n",
      "Testing loss: 1.4911561250686645\n",
      "3: LR = 0.0007\n",
      "Training loss: 1.4864128190417623\n",
      "Testing loss: 1.4824948489665986\n",
      "4: LR = 0.0007\n",
      "Training loss: 1.482742661653563\n",
      "Testing loss: 1.4811449348926544\n",
      "5: LR = 0.0006\n",
      "Training loss: 1.4770149990569714\n",
      "Testing loss: 1.4794403910636902\n",
      "6: LR = 0.0005\n",
      "Training loss: 1.4863087909166204\n",
      "Testing loss: 1.488053023815155\n",
      "7: LR = 0.0005\n",
      "Training loss: 1.4829520236614138\n",
      "Testing loss: 1.4747083902359008\n",
      "8: LR = 0.0004\n",
      "Training loss: 1.4780218240826628\n",
      "Testing loss: 1.4809668719768525\n",
      "9: LR = 0.0004\n",
      "Training loss: 1.4813127379084743\n",
      "Testing loss: 1.4765813887119292\n",
      "SWA Run 5\n",
      "0: LR = 0.0010\n",
      "Training loss: 1.4859735328097676\n",
      "Testing loss: 1.478007048368454\n",
      "1: LR = 0.0009\n",
      "Training loss: 1.4845859948978868\n",
      "Testing loss: 1.4783065736293792\n",
      "2: LR = 0.0008\n",
      "Training loss: 1.4827706813812256\n",
      "Testing loss: 1.4891422271728516\n",
      "3: LR = 0.0007\n",
      "Training loss: 1.4869645639907483\n",
      "Testing loss: 1.4831398725509644\n",
      "4: LR = 0.0007\n",
      "Training loss: 1.4842112480207932\n",
      "Testing loss: 1.488531094789505\n",
      "5: LR = 0.0006\n",
      "Training loss: 1.4853699068690456\n",
      "Testing loss: 1.4783201396465302\n",
      "6: LR = 0.0005\n",
      "Training loss: 1.4812888239705286\n",
      "Testing loss: 1.4800231337547303\n",
      "7: LR = 0.0005\n",
      "Training loss: 1.4811894727307697\n",
      "Testing loss: 1.4781607568264008\n",
      "8: LR = 0.0004\n",
      "Training loss: 1.4799303564914437\n",
      "Testing loss: 1.4771870732307435\n",
      "9: LR = 0.0004\n",
      "Training loss: 1.4777044251907703\n",
      "Testing loss: 1.474944418668747\n",
      "SWA Run 6\n",
      "0: LR = 0.0010\n",
      "Training loss: 1.482369675192722\n",
      "Testing loss: 1.476838093996048\n",
      "1: LR = 0.0009\n",
      "Training loss: 1.4868954392366631\n",
      "Testing loss: 1.4872447311878205\n",
      "2: LR = 0.0008\n",
      "Training loss: 1.4870743280233338\n",
      "Testing loss: 1.4902080833911895\n",
      "3: LR = 0.0007\n",
      "Training loss: 1.487131157586741\n",
      "Testing loss: 1.4866094946861268\n",
      "4: LR = 0.0007\n",
      "Training loss: 1.48340754453526\n",
      "Testing loss: 1.4753179490566253\n",
      "5: LR = 0.0006\n",
      "Training loss: 1.4796372596607652\n",
      "Testing loss: 1.4765394568443297\n",
      "6: LR = 0.0005\n",
      "Training loss: 1.4764676925747893\n",
      "Testing loss: 1.4773739874362946\n",
      "7: LR = 0.0005\n",
      "Training loss: 1.477415775143823\n",
      "Testing loss: 1.4787187576293945\n",
      "8: LR = 0.0004\n",
      "Training loss: 1.48046459430872\n",
      "Testing loss: 1.4743916749954225\n",
      "9: LR = 0.0004\n",
      "Training loss: 1.4802123585412668\n",
      "Testing loss: 1.4773676693439484\n",
      "SWA Run 7\n",
      "0: LR = 0.0010\n",
      "Training loss: 1.482645636381105\n",
      "Testing loss: 1.4894688248634338\n",
      "1: LR = 0.0009\n",
      "Training loss: 1.4843726962111716\n",
      "Testing loss: 1.4782385230064392\n",
      "2: LR = 0.0008\n",
      "Training loss: 1.480584094690722\n",
      "Testing loss: 1.48291836977005\n",
      "3: LR = 0.0007\n",
      "Training loss: 1.4850921270459196\n",
      "Testing loss: 1.4880284368991852\n",
      "4: LR = 0.0007\n",
      "Training loss: 1.4822174671084383\n",
      "Testing loss: 1.4777629792690277\n",
      "5: LR = 0.0006\n",
      "Training loss: 1.4836393372957097\n",
      "Testing loss: 1.4795490026473999\n",
      "6: LR = 0.0005\n",
      "Training loss: 1.4778584629990335\n",
      "Testing loss: 1.4779103755950929\n",
      "7: LR = 0.0005\n",
      "Training loss: 1.4778853017230367\n",
      "Testing loss: 1.4737631797790527\n",
      "8: LR = 0.0004\n",
      "Training loss: 1.4798140276310057\n",
      "Testing loss: 1.4778444826602937\n",
      "9: LR = 0.0004\n",
      "Training loss: 1.8613424827886182\n",
      "Testing loss: 2.302527356147766\n",
      "SWA Run 8\n",
      "0: LR = 0.0010\n",
      "Training loss: 2.3024175388868464\n",
      "Testing loss: 2.302307736873627\n",
      "1: LR = 0.0009\n",
      "Training loss: 2.302398925603822\n",
      "Testing loss: 2.302157092094421\n",
      "2: LR = 0.0008\n",
      "Training loss: 2.3022961782854656\n",
      "Testing loss: 2.3020148873329163\n",
      "3: LR = 0.0007\n",
      "Training loss: 2.3021005308905313\n",
      "Testing loss: 2.302138864994049\n",
      "4: LR = 0.0007\n",
      "Training loss: 2.3021700825802114\n",
      "Testing loss: 2.3021812438964844\n",
      "5: LR = 0.0006\n",
      "Training loss: 2.302259683609009\n",
      "Testing loss: 2.302122950553894\n",
      "6: LR = 0.0005\n",
      "Training loss: 2.3019973178242528\n",
      "Testing loss: 2.302175223827362\n",
      "7: LR = 0.0005\n",
      "Training loss: 2.3020594064579454\n",
      "Testing loss: 2.301979100704193\n",
      "8: LR = 0.0004\n",
      "Training loss: 2.302097758581472\n",
      "Testing loss: 2.3020148873329163\n",
      "9: LR = 0.0004\n",
      "Training loss: 2.301892275033995\n",
      "Testing loss: 2.302391993999481\n",
      "SWA Run 9\n",
      "0: LR = 0.0010\n",
      "Training loss: 2.3019210516020308\n",
      "Testing loss: 2.3017274498939515\n",
      "1: LR = 0.0009\n",
      "Training loss: 2.3017468563345975\n",
      "Testing loss: 2.302013087272644\n",
      "2: LR = 0.0008\n",
      "Training loss: 2.3016576378844507\n",
      "Testing loss: 2.3022802114486693\n",
      "3: LR = 0.0007\n",
      "Training loss: 2.301833513171174\n",
      "Testing loss: 2.3014859795570373\n",
      "4: LR = 0.0007\n",
      "Training loss: 2.3016698914904925\n",
      "Testing loss: 2.3019498705863954\n",
      "5: LR = 0.0006\n",
      "Training loss: 2.3021921446157054\n",
      "Testing loss: 2.3018648862838744\n",
      "6: LR = 0.0005\n",
      "Training loss: 2.302110888237177\n",
      "Testing loss: 2.3022075057029725\n",
      "7: LR = 0.0005\n",
      "Training loss: 2.3019591264946517\n",
      "Testing loss: 2.3012608647346497\n",
      "8: LR = 0.0004\n",
      "Training loss: 2.3016102036764456\n",
      "Testing loss: 2.3018720507621766\n",
      "9: LR = 0.0004\n",
      "Training loss: 2.3016683389974193\n",
      "Testing loss: 2.301455187797546\n"
     ]
    }
   ],
   "source": [
    "for swa_run in range(10):\n",
    "    print(\"SWA Run {}\".format(swa_run))\n",
    "    for epoch in range(10):\n",
    "        losses = []\n",
    "        test_losses = []\n",
    "        lr = adjust_lr(optimizer, epoch)\n",
    "        print(f\"{epoch}: LR = {lr:.4f}\")\n",
    "        model.train()\n",
    "        for batchx, batchy in training_data_generator(trainx, trainy, batch_size = 128, subsample_pct = .1):\n",
    "            data = torch.FloatTensor(batchx).requires_grad_(True)\n",
    "            labels = torch.LongTensor(batchy.argmax(1))\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, labels)\n",
    "            losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        for batchx, batchy in training_data_generator(testx, testy, batch_size = 128, subsample_pct = .25):\n",
    "            data = torch.FloatTensor(batchx)\n",
    "            labels = torch.LongTensor(batchy.argmax(1))\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, labels)\n",
    "            test_losses.append(loss.item())\n",
    "        print(\"Training loss: {}\".format(np.mean(losses)))\n",
    "        print(\"Testing loss: {}\".format(np.mean(test_losses)))\n",
    "    torch.save(model.state_dict(), open(f\"swa_run{swa_run}.pth\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([ 0.9998,  0.9999,  1.0000,  1.0000,  1.0000]),\n",
       "  tensor([ 7,  3,  4,  6,  1])),\n",
       " array([7, 3, 4, ..., 5, 6, 8], dtype=int64))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.FloatTensor(trainx[:5])).max(1), trainy.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_predict(model, testx, batch_size = 128):\n",
    "    outputs = []\n",
    "    for i in range(ceil(len(testx) / batch_size)):\n",
    "        data = torch.FloatTensor(testx[i * batch_size : (i + 1) * batch_size])\n",
    "        preds = model(data)\n",
    "        outputs.append(preds.detach().numpy())\n",
    "    return outputs\n",
    "\n",
    "preds = batch_predict(model, testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.concatenate(preds)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9705"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds.argmax(1) == testy.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['swa_run0.pth',\n",
       " 'swa_run1.pth',\n",
       " 'swa_run2.pth',\n",
       " 'swa_run3.pth',\n",
       " 'swa_run4.pth',\n",
       " 'swa_run5.pth',\n",
       " 'swa_run6.pth']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "weights_files = [filename for filename in os.listdir() if filename.startswith(\"swa\")]\n",
    "weights_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for filename in weights_files:\n",
    "    model = MNISTNet()\n",
    "    model.load_state_dict(torch.load(open(filename, \"rb\")))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "map"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = map(lambda x: batch_predict(x, testx), models)\n",
    "type(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = list(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lesspreds = [np.concatenate(sublist) for sublist in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 10), 7)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesspreds[0].shape, len(lesspreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10000, 10)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalpreds = np.array(lesspreds)\n",
    "finalpreds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9879"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(finalpreds.mean(0).argmax(1) == testy.argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Single model:  \n",
    "  * 97.05% accuracy\n",
    "* 7 model ensemble:  \n",
    "  * 98.79% accuracy\n",
    "* SWA model:\n",
    "  * 98.76% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swa_model = MNISTNet()\n",
    "for param in swa_model.parameters():\n",
    "    param.data.fill_(0)\n",
    "for i, filename in enumerate(weights_files):\n",
    "    params = torch.load(open(filename, \"rb\"))\n",
    "    for name, param in swa_model.named_parameters():\n",
    "        param.data = param.data * (i / (i + 1)) + params[name].data * (1 / (i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = batch_predict(swa_model, testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.concatenate(preds)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9876"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds.argmax(1) == testy.argmax(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:retro]",
   "language": "python",
   "name": "conda-env-retro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
